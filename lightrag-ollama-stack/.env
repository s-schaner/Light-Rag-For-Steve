# Bind both LLM and embeddings to Ollama
LLM_BINDING=ollama
EMBEDDING_BINDING=ollama

# Inside-docker Ollama URL (service name)
LLM_BINDING_HOST=http://ollama:11434
EMBEDDING_BINDING_HOST=http://ollama:11434

# Models (tweak as you like)
LLM_MODEL=qwen2.5:7b-instruct
EMBEDDING_MODEL=nomic-embed-text

# LightRAG working folders (server will create if missing)
INPUTS_DIR=/app/inputs
RAG_DIR=/app/rag_storage
WORKING_DIR=/app/rag_storage

# Optional perf knobs
LLM_MODEL_MAX_ASYNC=4
